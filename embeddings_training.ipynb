{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a392fc55-f1b5-4e07-826e-6ca17f87b2d6",
   "metadata": {
    "id": "a392fc55-f1b5-4e07-826e-6ca17f87b2d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0627e-0f9f-4ca7-9ee4-5ed16aa4e7fc",
   "metadata": {
    "id": "ecc0627e-0f9f-4ca7-9ee4-5ed16aa4e7fc"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5391e9bc-dbc1-481c-9abf-f41973929324",
   "metadata": {
    "id": "5391e9bc-dbc1-481c-9abf-f41973929324"
   },
   "outputs": [],
   "source": [
    "def load_corpus(path):\n",
    "  corpus = list()\n",
    "\n",
    "  with open(path, \"r\") as f:\n",
    "    corpus = [line.replace(\" \", \"\").replace(\"\\n\", \"\").split(\"|\") for line in f.readlines()]\n",
    "    corpus = [ [chord for chord in seq if len(chord) > 0] for seq in corpus if len(seq) > 15 ]\n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a61463-b850-4bd1-9771-1904b1a07572",
   "metadata": {
    "id": "10a61463-b850-4bd1-9771-1904b1a07572"
   },
   "outputs": [],
   "source": [
    "train = load_corpus(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb9236-a2fd-44db-8e7b-449c9441c584",
   "metadata": {
    "id": "0abb9236-a2fd-44db-8e7b-449c9441c584"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b2c6f4-9dda-4d9a-a1c1-26bd503bc83b",
   "metadata": {
    "cellView": "form",
    "id": "58b2c6f4-9dda-4d9a-a1c1-26bd503bc83b"
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "\n",
    "t = 1e-5 #@param {type: \"number\"}\n",
    "c = 2 #@param {type: \"number\"}\n",
    "k = 20 #@param {type: \"number\"}\n",
    "alpha = 1 #@param {type: \"number\"}\n",
    "epochs = 100 #@param {type: \"number\"}\n",
    "\n",
    "batch_size = 2048 #@param {type: \"number\"}\n",
    "embedding_dim = 300 #@param {type: \"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b1ed1-83cf-4e62-9df6-5915f049fde4",
   "metadata": {
    "id": "1b2b1ed1-83cf-4e62-9df6-5915f049fde4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## word2vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3f7bb4-6b90-470b-9aa8-2792572647b5",
   "metadata": {
    "id": "7d3f7bb4-6b90-470b-9aa8-2792572647b5"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=train, \n",
    "                 vector_size=embedding_dim, \n",
    "                 window=c, \n",
    "                 min_count=0, \n",
    "                 workers=4,\n",
    "                 hs=0,\n",
    "                 negative=k,\n",
    "                 ns_exponent=alpha,\n",
    "                 sample=t,\n",
    "                 epochs=epochs,\n",
    "                 seed=RANDOM_SEED)\n",
    "\n",
    "model.save(\"word2vec.gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a4e60-7c01-46a8-80ea-6de1984a4066",
   "metadata": {
    "id": "7d7a4e60-7c01-46a8-80ea-6de1984a4066",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## fasttext training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea93fcc-3a44-4646-ba6c-6cbd714c2599",
   "metadata": {
    "id": "1ea93fcc-3a44-4646-ba6c-6cbd714c2599"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(sentences=train, \n",
    "                 vector_size=embedding_dim, \n",
    "                 window=c, \n",
    "                 min_count=0, \n",
    "                 workers=4,\n",
    "                 hs=0,\n",
    "                 negative=k,\n",
    "                 ns_exponent=alpha,\n",
    "                 sample=t,\n",
    "                 epochs=epochs,\n",
    "                 seed=RANDOM_SEED)\n",
    "\n",
    "model.save(\"fasttext.gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad3dbd-ee17-4fa0-8283-20e279530738",
   "metadata": {
    "id": "0bad3dbd-ee17-4fa0-8283-20e279530738",
    "tags": []
   },
   "source": [
    "## harte2vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d490670a-73d2-458a-983e-46ec1afc1387",
   "metadata": {
    "id": "d490670a-73d2-458a-983e-46ec1afc1387"
   },
   "outputs": [],
   "source": [
    "from harte2vec.harte import HarteToIntervals\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "def onehot_intervals(intervals):\n",
    "  if len(intervals) == 0:\n",
    "    chord_encoding = np.array([0])\n",
    "  else:\n",
    "    intervals_encoding = np.stack([np.eye(12)[i, :].sum(axis=0).clip(0, 1) for i in intervals])\n",
    "    whole_chord_encoding = intervals_encoding.sum(axis=0).clip(0, 1)\n",
    "    chord_encoding = np.vstack((intervals_encoding, whole_chord_encoding))\n",
    "    # convert chord encoding to indexes\n",
    "    chord_encoding = chord_encoding.dot(2**np.arange(12)[::-1]).astype(int)\n",
    "  \n",
    "  return chord_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88e544-d900-4e8c-8fbe-cf9d85958a76",
   "metadata": {
    "id": "6b88e544-d900-4e8c-8fbe-cf9d85958a76"
   },
   "source": [
    "Subsample data according to \n",
    "\n",
    "$$ p(c) = \\frac{f(c) - t}{f(c)} - \\sqrt{\\frac{t}{f(c)}} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ P(c) = \\frac{f(c)^\\alpha}{\\sum_{c'} f(c')^\\alpha} $$.\n",
    "\n",
    "We will set $\\alpha = -0.5$, $t = 10^{-5}$, $100$ epochs and a context size of $3$.\n",
    "\n",
    "\n",
    "[1] https://arxiv.org/pdf/1804.04212.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8141f5-78c8-4164-9c4d-d7f56ad4fa06",
   "metadata": {
    "id": "0b8141f5-78c8-4164-9c4d-d7f56ad4fa06"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def subsample_corpus(data: List[List[str]], freq: np.ndarray, vocab: List[str], alpha: float = -0.5, t: float = 1e-5) -> List[List[str]]:\n",
    "  # subsample data according to distribution\n",
    "  P_c = freq**alpha / sum(freq**alpha)\n",
    "  p_c = ((freq - t) / freq) - np.sqrt(t / freq)\n",
    "  p_c = dict(zip(vocab, p_c))\n",
    "  # subsample corpus\n",
    "  subsampled_data = [[sample for sample in doc if np.random.random() < p_c[sample]] for doc in data]\n",
    "  return subsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9288d-cab3-4279-a3ef-528523e3cdbc",
   "metadata": {
    "id": "bbf9288d-cab3-4279-a3ef-528523e3cdbc"
   },
   "source": [
    "During training each sample will be augmented with positive and negative samples to ease the training phase. Positive samples are all those items that are in the context window. Negative samples are selected randomly from the dataset based on their probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e164d117-72e1-4e6c-9d6c-3c9270715bc6",
   "metadata": {
    "id": "e164d117-72e1-4e6c-9d6c-3c9270715bc6"
   },
   "outputs": [],
   "source": [
    "def negative_sampling(data: List[List[str]], \n",
    "                      vocab: List[str], \n",
    "                      c: int = 3, \n",
    "                      k: int = 15):\n",
    "    # implementation based on \n",
    "    # https://tech.hbc.com/2018-03-23-negative-sampling-in-numpy.html\n",
    "    context_size = 2 * c + 1\n",
    "\n",
    "    # needs to remove initial and final element without contigous context\n",
    "    num_samples = len(data) - (2*c)\n",
    "    X = np.zeros(num_samples, dtype=int)\n",
    "    y_pos = np.zeros((num_samples, context_size), dtype=int)\n",
    "    y_neg = np.zeros((num_samples, k), dtype=int)\n",
    "\n",
    "    data_idxs = np.searchsorted(vocab, data)\n",
    "    window_idxs = np.lib.stride_tricks.sliding_window_view(data_idxs, context_size)\n",
    "\n",
    "    for i, positives_index in enumerate(window_idxs):\n",
    "        # extract positives and central chord idx\n",
    "        element_idx = positives_index[context_size // 2]\n",
    "\n",
    "        raw_samp = np.random.randint(0, len(vocab) - context_size, size=k)\n",
    "        pos_idxs_adj = positives_index - np.arange(len(positives_index))\n",
    "        ss = np.searchsorted(pos_idxs_adj, raw_samp, side='right')\n",
    "        negatives_index = raw_samp + ss\n",
    "\n",
    "        X[i] = element_idx\n",
    "        y_pos[i] = positives_index\n",
    "        y_neg[i] = negatives_index\n",
    "\n",
    "    return X, y_pos, y_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca01edb-c567-4bfa-867f-42560ddce904",
   "metadata": {
    "id": "fca01edb-c567-4bfa-867f-42560ddce904"
   },
   "source": [
    "Create torch Dataset for easier training and batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a578ae6-18ab-411a-ab1b-8c96635b0116",
   "metadata": {
    "id": "6a578ae6-18ab-411a-ab1b-8c96635b0116"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from more_itertools import windowed, collapse\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ChocoHarte2VecDataset(Dataset):\n",
    "  def __init__(self, data: List[str], t: float = 1e-5, c: int = 3, alpha: float = -0.5, k: int = 20):\n",
    "    super().__init__()\n",
    "    self._data = data\n",
    "    self.t = t\n",
    "    self.c = c\n",
    "    self.alpha = alpha\n",
    "    self.k = k\n",
    "\n",
    "    # build vocab and sort to enable faster numpy searching\n",
    "    counter = dict(sorted(Counter(chain(*self._data)).items(), key=lambda item: item[0]))\n",
    "\n",
    "    # turn into numpy array for faster searching\n",
    "    self.vocab = np.array(list(counter.keys()))\n",
    "    self.freq = np.array([counter[chord] for chord in self.vocab])\n",
    "    \n",
    "    # compute one hot encoding of whole vocabulary\n",
    "    # use numpy array for faster searching\n",
    "    self.h2i = HarteToIntervals()\n",
    "    self.intervals_encoding = np.array([onehot_intervals(self.h2i.convert(chord)) for chord in self.vocab])\n",
    "    self._data = subsample_corpus(self._data, self.freq, self.vocab, alpha=self.alpha, t=self.t)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    data_item = self._data[index]\n",
    "    # perform negative sampling\n",
    "    X, pos, neg = negative_sampling(data_item, self.vocab, c=self.c, k=self.k)\n",
    "    \n",
    "    X = np.take(self.intervals_encoding, X)\n",
    "    pos = np.take(self.intervals_encoding, pos)\n",
    "    neg = np.take(self.intervals_encoding, neg)\n",
    "    \n",
    "    X_pos = [np.tile(xi, (len(pos_i), 1)) for xi, pos_i in zip(X, pos)]\n",
    "    X_neg = [np.tile(xi, (len(neg_i), 1)) for xi, neg_i in zip(X, neg)]\n",
    "    \n",
    "    return X_pos, pos, X_neg, neg\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._data)\n",
    "\n",
    "  def _collate_seq(self, seq):\n",
    "    return pad_sequence(list(map(torch.tensor, (collapse(seq, levels=2)))),\n",
    "                        batch_first=True,\n",
    "                        padding_value=0)\n",
    "\n",
    "  def collate_fn(self, samples):\n",
    "    X_pos, y_pos, X_neg, y_neg = zip(*samples)\n",
    "\n",
    "    X_pos = self._collate_seq(X_pos)\n",
    "    y_pos = self._collate_seq(y_pos)\n",
    "    X_neg = self._collate_seq(X_neg)\n",
    "    y_neg = self._collate_seq(y_neg)\n",
    "    \n",
    "    return X_pos, y_pos, X_neg, y_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab87979-acc3-4019-8533-4c4476af7b2e",
   "metadata": {
    "id": "0ab87979-acc3-4019-8533-4c4476af7b2e"
   },
   "source": [
    "Define the model as in the original fasttext paper [1]\n",
    "\n",
    "[1] https://arxiv.org/abs/1607.04606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde72350-783b-4a1e-93e4-3c70a7d90114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Harte2VecModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.full_vocab_size = 2**12 + 1\n",
    "        self.embedding = nn.EmbeddingBag(self.full_vocab_size, self.embedding_dim, mode=\"sum\")\n",
    "    \n",
    "    def _predict(self, source, target):\n",
    "        source = self.embedding(source)\n",
    "        target = self.embedding(target)\n",
    "        y = torch.einsum(\"ij,ik->i\", source, target)\n",
    "        return y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_pos, y_pos, x_neg, y_neg = batch\n",
    "        \n",
    "        pos_pred = self._predict(x_pos, y_pos)\n",
    "        neg_pred = self._predict(x_neg, y_neg)\n",
    "        pred = torch.cat([pos_pred, neg_pred])\n",
    "        target = torch.cat([\n",
    "            torch.ones_like(pos_pred, device=pos_pred.device),\n",
    "            torch.zeros_like(neg_pred, device=neg_pred.device)\n",
    "        ])\n",
    "    \n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(pred, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.025)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f84aae4-d8ce-4b76-84e5-a7cb58092119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, batch_size: int = 512):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, collate_fn=self.train.collate_fn, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51029d-c167-4273-83d3-eb66d6cbcd55",
   "metadata": {
    "id": "ea51029d-c167-4273-83d3-eb66d6cbcd55"
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb7a9a9-8fce-490a-8394-c2aff79a36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 #@param {type: \"number\"}\n",
    "embedding_dim = 10 #@param {type: \"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "834bf540-e133-477a-950f-a854322e7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4135889/4147086510.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.intervals_encoding = np.array([onehot_intervals(self.h2i.convert(chord)) for chord in self.vocab])\n"
     ]
    }
   ],
   "source": [
    "dataset = ChocoHarte2VecDataset(train, t=t, alpha=alpha, c=c, k=k)\n",
    "data = DataModule(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f875c743-da92-4075-bf49-5b3a8ef9b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | embedding | EmbeddingBag | 41.0 K\n",
      "-------------------------------------------\n",
      "41.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.0 K    Total params\n",
      "0.164     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a45709e6b1417f80206ceeec698440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "model = Harte2VecModel(embedding_dim=embedding_dim)\n",
    "    \n",
    "trainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "                     ])\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "HwdrseO1zR7m",
   "metadata": {
    "id": "HwdrseO1zR7m"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "  \"embedding\": model.embedding.weight,\n",
    "  \"vocab\": dataset.vocab\n",
    "}, \"harte2vec.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01a7b335e0a74e83aa52e2f213ce149a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3761e591be5e4f7db440b67c3151d52d",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39dd93ea79e8421cbde94b32ba6dc7d9",
      "value": 15
     }
    },
    "112772cfccf54e5886831958672e405a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c821fe9e9794446a2098201ce4aeb49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fe9d0729252418781ca16750be15cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c821fe9e9794446a2098201ce4aeb49",
      "placeholder": "​",
      "style": "IPY_MODEL_862c8f0b087b4491bea83606290af35b",
      "value": "Step    15 / 15: 100%"
     }
    },
    "3761e591be5e4f7db440b67c3151d52d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39dd93ea79e8421cbde94b32ba6dc7d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6be9e4ed949c4dc69e992fcbbb614087": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "862c8f0b087b4491bea83606290af35b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a940af280b3540959ddce59be16e19e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2fe9d0729252418781ca16750be15cf3",
       "IPY_MODEL_01a7b335e0a74e83aa52e2f213ce149a",
       "IPY_MODEL_e41626484a7b4819addd8b8a7eddbb07"
      ],
      "layout": "IPY_MODEL_112772cfccf54e5886831958672e405a"
     }
    },
    "d3f16eac8e1e47bc837033f09e95cebd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e41626484a7b4819addd8b8a7eddbb07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3f16eac8e1e47bc837033f09e95cebd",
      "placeholder": "​",
      "style": "IPY_MODEL_6be9e4ed949c4dc69e992fcbbb614087",
      "value": " 15/15 [13:19&lt;00:00, 52.84s/it, loss=0.0266]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
