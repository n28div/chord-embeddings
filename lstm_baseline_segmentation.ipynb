{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sqn_Jmp7OQK",
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "er9gPVlMlF4n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"segmentation_dataset.npz\", \"rb\") as f:\n",
    "  npz = np.load(f, allow_pickle=True)\n",
    "  X_train, y_train, X_valid, y_valid, X_test, y_test = npz.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad all the sequences to the longest sequence in the split and encode them using the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = list(set.union(*[set(yi) for yi in y_train]))\n",
    "encoder = OneHotEncoder().fit(np.array(labels).reshape(-1, 1))\n",
    "\n",
    "def embed_data(model, X, y, encoder):\n",
    "    seq_length = [len(xi) for xi in X]\n",
    "    max_seq_length = max(seq_length)\n",
    "    \n",
    "    padded_seq = np.stack(\n",
    "        [np.pad(xi, (0, max_seq_length - len(xi)), constant_values=\"P\") \n",
    "         for xi in X])\n",
    "    \n",
    "    padding_mask = torch.tensor((padded_seq == \"P\").astype(int))\n",
    "    padded_seq[padding_mask == 1] = \"N\"\n",
    "    \n",
    "    embedded_seq = torch.tensor(np.stack([[model[sample]\n",
    "                                         for sample in seq] \n",
    "                                        for seq in padded_seq]))\n",
    "    \n",
    "    \n",
    "    encoded_labels = [torch.tensor(encoder.transform(yi.reshape(-1, 1)).todense()) for yi in y]\n",
    "    padded_labels = pad_sequence(encoded_labels, batch_first=True, padding_value=0)\n",
    "  \n",
    "    return embedded_seq, padded_labels, padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwrXF12H8R5x",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XWizPjGUmLhG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from segmentation.metrics import pairwise_metrics, under_over_segmentation\n",
    "from collections import defaultdict\n",
    "from more_itertools import stagger\n",
    "\n",
    "class BaselineModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 10,\n",
    "                 hidden_size: int = 100, \n",
    "                 dropout: float = 0.0,\n",
    "                 num_layers: int = 1, \n",
    "                 num_labels: int = 10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size,\n",
    "                            dropout=dropout,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.classification = nn.Linear(hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def _predict(self, batch):\n",
    "        x, y, mask = batch\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.classification(x)\n",
    "        x = self.softmax(x)\n",
    "               \n",
    "        loss = nn.functional.binary_cross_entropy(x[mask == 0].float(), y[mask == 0].float())\n",
    "        return x, loss\n",
    "    \n",
    "    def _postprocess(self, pred):\n",
    "        for i, (p, c, n) in enumerate(stagger(pred)):\n",
    "            if p != None and p != c != n:\n",
    "                pred[i] = n\n",
    "        return pred\n",
    "    \n",
    "    def _test(self, batch):\n",
    "        metrics = defaultdict(list)\n",
    "        x, y, mask = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred, loss = self._predict(batch)\n",
    "            \n",
    "            for pi, yi, mi in zip(pred, y, mask):\n",
    "                pi = pi[mi == 0].argmax(axis=-1).cpu().numpy()\n",
    "                _, pi = np.unique(pi, return_inverse=True)\n",
    "            \n",
    "                yi = yi[mi == 0].argmax(axis=-1).cpu().numpy()\n",
    "                _, yi = np.unique(yi, return_inverse=True)\n",
    "            \n",
    "                precision, recall, f1 = pairwise_metrics(yi, pi)\n",
    "                metrics[\"precision\"].append(precision)\n",
    "                metrics[\"recall\"].append(recall)\n",
    "                metrics[\"f1\"].append(f1)\n",
    "                under, over = under_over_segmentation(yi, pi)\n",
    "                metrics[\"under\"] = under\n",
    "                metrics[\"over\"] = over\n",
    "        \n",
    "        metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "        return loss, metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss = self._predict(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, metrics = self._test(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        for k, m in metrics.items(): self.log(f\"val_{k}\", m)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, metrics = self._test(batch)        \n",
    "        self.log(\"test_loss\", loss)\n",
    "        for k, m in metrics.items(): self.log(f\"test_{k}\", m)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, valid = None, test = None, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self._train = train\n",
    "        self._valid = valid\n",
    "        self._test = test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(list(zip(*self._train)), batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self._valid is not None:\n",
    "            return DataLoader(list(zip(*self._valid)), batch_size=self.batch_size)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        if self._test is not None:\n",
    "            return DataLoader(list(zip(*self._test)), batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG50CVxK8cme",
    "tags": []
   },
   "source": [
    "## Train using harte2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "harte2vec = Harte2Vec.from_pretrimport pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under\"] + df[\"val_over\"] + df[\"val_f1\"]).sort_values(ascending=False).index]ained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_data(harte2vec, X_train, y_train, encoder), \n",
    "                  embed_data(harte2vec, X_valid, y_valid, encoder), \n",
    "                  embed_data(harte2vec, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "def train(config, data=data, epochs=500, validate=True, log=False):\n",
    "    model = BaselineModel(**config)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1,\n",
    "                         enable_progress_bar=False,\n",
    "                         callbacks=[\n",
    "                             EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                             StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "                         ])\n",
    "    trainer.fit(model, datamodule=data)\n",
    "    results = trainer.validate(model, datamodule=data, verbose=False) if validate else None\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d247519e7840f1868c6758afeed6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [2, 5, 10],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.5],\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=350)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246221</td>\n",
       "      <td>0.453463</td>\n",
       "      <td>0.940055</td>\n",
       "      <td>0.591652</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.387198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235573</td>\n",
       "      <td>0.510368</td>\n",
       "      <td>0.897013</td>\n",
       "      <td>0.624548</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238436</td>\n",
       "      <td>0.491842</td>\n",
       "      <td>0.923419</td>\n",
       "      <td>0.620334</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234461</td>\n",
       "      <td>0.486818</td>\n",
       "      <td>0.933336</td>\n",
       "      <td>0.617251</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256420</td>\n",
       "      <td>0.461730</td>\n",
       "      <td>0.954748</td>\n",
       "      <td>0.601870</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.248866</td>\n",
       "      <td>0.461730</td>\n",
       "      <td>0.954748</td>\n",
       "      <td>0.601870</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.247240</td>\n",
       "      <td>0.461730</td>\n",
       "      <td>0.954748</td>\n",
       "      <td>0.601870</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.265206</td>\n",
       "      <td>0.415539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.243349</td>\n",
       "      <td>0.461484</td>\n",
       "      <td>0.944391</td>\n",
       "      <td>0.595604</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>0.306476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.425210</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.579649</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250899</td>\n",
       "      <td>0.456378</td>\n",
       "      <td>0.910024</td>\n",
       "      <td>0.584356</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.287154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.247999</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.936134</td>\n",
       "      <td>0.596523</td>\n",
       "      <td>0.646757</td>\n",
       "      <td>0.386688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  val_loss  val_precision  val_recall  \\\n",
       "2             2          10      0.0  0.246221       0.453463    0.940055   \n",
       "8            10           5      0.0  0.235573       0.510368    0.897013   \n",
       "6             5          10      0.0  0.238436       0.491842    0.923419   \n",
       "10           10          10      0.0  0.234461       0.486818    0.933336   \n",
       "0             2           5      0.0  0.256420       0.461730    0.954748   \n",
       "7             5          10      0.5  0.248866       0.461730    0.954748   \n",
       "11           10          10      0.5  0.247240       0.461730    0.954748   \n",
       "3             2          10      0.5  0.265206       0.415539    1.000000   \n",
       "9            10           5      0.5  0.243349       0.461484    0.944391   \n",
       "1             2           5      0.5  0.262745       0.425210    0.997376   \n",
       "4             5           5      0.0  0.250899       0.456378    0.910024   \n",
       "5             5           5      0.5  0.247999       0.463636    0.936134   \n",
       "\n",
       "      val_f1  val_under  val_over  \n",
       "2   0.591652   0.956955  0.387198  \n",
       "8   0.624548   0.900554  0.362908  \n",
       "6   0.620334   0.900554  0.362908  \n",
       "10  0.617251   0.900554  0.362908  \n",
       "0   0.601870   0.900554  0.362908  \n",
       "7   0.601870   0.900554  0.362908  \n",
       "11  0.601870   0.900554  0.362908  \n",
       "3   0.571265   1.000000  0.212421  \n",
       "9   0.595604   0.881099  0.306476  \n",
       "1   0.579649   0.956955  0.243891  \n",
       "4   0.584356   0.900554  0.287154  \n",
       "5   0.596523   0.646757  0.386688  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under\"] + df[\"val_over\"] + df[\"val_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_data(harte2vec, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(harte2vec, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee89853b26049d69006eabfc0d5bd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BaselineModel(hidden_size=5, num_layers=10, dropout=0.0)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/harte2vec_lstm\",\n",
    "                     max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", patience=10),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a60f50086174045bf43523ece4dfabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_f1            0.5716114712680601\n",
      "        test_loss           0.2264181226491928\n",
      "        test_over           0.5705736238317893\n",
      "     test_precision         0.4536421055554555\n",
      "       test_recall          0.8340693963702562\n",
      "       test_under           0.9521838657630957\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(word2vec.wv, X_train, y_train, encoder), \n",
    "                  embed_data(word2vec.wv, X_valid, y_valid, encoder), \n",
    "                  embed_data(word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c300560e55734e37878ec4b66c350d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "    \"embedding_dim\": [word2vec.vector_size]\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=500)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.238776</td>\n",
       "      <td>0.504178</td>\n",
       "      <td>0.889871</td>\n",
       "      <td>0.621914</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.243439</td>\n",
       "      <td>0.465856</td>\n",
       "      <td>0.938855</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.241027</td>\n",
       "      <td>0.465856</td>\n",
       "      <td>0.938855</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235364</td>\n",
       "      <td>0.469606</td>\n",
       "      <td>0.914642</td>\n",
       "      <td>0.594758</td>\n",
       "      <td>0.937257</td>\n",
       "      <td>0.399985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.230996</td>\n",
       "      <td>0.484252</td>\n",
       "      <td>0.942096</td>\n",
       "      <td>0.616464</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.236915</td>\n",
       "      <td>0.474295</td>\n",
       "      <td>0.919815</td>\n",
       "      <td>0.603955</td>\n",
       "      <td>0.895268</td>\n",
       "      <td>0.404294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.238449</td>\n",
       "      <td>0.463079</td>\n",
       "      <td>0.953341</td>\n",
       "      <td>0.602682</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235693</td>\n",
       "      <td>0.462152</td>\n",
       "      <td>0.953432</td>\n",
       "      <td>0.601858</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.228508</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.900951</td>\n",
       "      <td>0.602858</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.364232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.250593</td>\n",
       "      <td>0.472174</td>\n",
       "      <td>0.907787</td>\n",
       "      <td>0.598567</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>0.398160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.237412</td>\n",
       "      <td>0.456865</td>\n",
       "      <td>0.968679</td>\n",
       "      <td>0.600971</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.239274</td>\n",
       "      <td>0.457102</td>\n",
       "      <td>0.966241</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.237769</td>\n",
       "      <td>0.479266</td>\n",
       "      <td>0.794916</td>\n",
       "      <td>0.578413</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.314855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.265730</td>\n",
       "      <td>0.415539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.265679</td>\n",
       "      <td>0.415539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.256425</td>\n",
       "      <td>0.448285</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.594902</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>0.306476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.261281</td>\n",
       "      <td>0.424127</td>\n",
       "      <td>0.997430</td>\n",
       "      <td>0.578537</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.260187</td>\n",
       "      <td>0.440054</td>\n",
       "      <td>0.984019</td>\n",
       "      <td>0.589996</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.287154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  embedding_dim  val_loss  val_precision  \\\n",
       "13          200           5      0.3            300  0.238776       0.504178   \n",
       "4           100          10      0.3            300  0.243439       0.465856   \n",
       "16          200          10      0.3            300  0.241027       0.465856   \n",
       "1           100           5      0.3            300  0.235364       0.469606   \n",
       "2           100           5      0.5            300  0.230996       0.484252   \n",
       "3           100          10      0.0            300  0.236915       0.474295   \n",
       "17          200          10      0.5            300  0.238449       0.463079   \n",
       "11          150          10      0.5            300  0.235693       0.462152   \n",
       "7           150           5      0.3            300  0.228508       0.479463   \n",
       "15          200          10      0.0            300  0.250593       0.472174   \n",
       "9           150          10      0.0            300  0.237412       0.456865   \n",
       "12          200           5      0.0            300  0.239274       0.457102   \n",
       "0           100           5      0.0            300  0.237769       0.479266   \n",
       "10          150          10      0.3            300  0.265730       0.415539   \n",
       "14          200           5      0.5            300  0.265679       0.415539   \n",
       "5           100          10      0.5            300  0.256425       0.448285   \n",
       "6           150           5      0.0            300  0.261281       0.424127   \n",
       "8           150           5      0.5            300  0.260187       0.440054   \n",
       "\n",
       "    val_recall    val_f1  val_under  val_over  \n",
       "13    0.889871  0.621914   0.953125  0.422784  \n",
       "4     0.938855  0.601375   0.953125  0.422784  \n",
       "16    0.938855  0.601375   0.953125  0.422784  \n",
       "1     0.914642  0.594758   0.937257  0.399985  \n",
       "2     0.942096  0.616464   0.917540  0.375694  \n",
       "3     0.919815  0.603955   0.895268  0.404294  \n",
       "17    0.953341  0.602682   0.917540  0.375694  \n",
       "11    0.953432  0.601858   0.917540  0.375694  \n",
       "7     0.900951  0.602858   0.924982  0.364232  \n",
       "15    0.907787  0.598567   0.857806  0.398160  \n",
       "9     0.968679  0.600971   0.905265  0.342394  \n",
       "12    0.966241  0.600600   0.905265  0.342394  \n",
       "0     0.794916  0.578413   0.905265  0.314855  \n",
       "10    1.000000  0.571265   1.000000  0.212421  \n",
       "14    1.000000  0.571265   1.000000  0.212421  \n",
       "5     0.973658  0.594902   0.881099  0.306476  \n",
       "6     0.997430  0.578537   0.956955  0.243891  \n",
       "8     0.984019  0.589996   0.900554  0.287154  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under\"] + df[\"val_over\"] + df[\"val_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(word2vec.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:374: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "  rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5c410df1b4952b11fc2aa41b0a5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "model = BaselineModel(hidden_size=200, num_layers=5, dropout=0.3, embedding_dim=word2vec.vector_size)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/word2vec_lstm\",\n",
    "                     max_epochs=500, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=5),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54579cf23cd04d09811e757f5996162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_f1            0.5612626493466071\n",
      "        test_loss           0.22821615636348724\n",
      "        test_over           0.5890391885847057\n",
      "     test_precision         0.4264151985527739\n",
      "       test_recall          0.9138498778367697\n",
      "       test_under           0.9296958362175696\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(fasttext.wv, X_train, y_train, encoder), \n",
    "                  embed_data(fasttext.wv, X_valid, y_valid, encoder), \n",
    "                  embed_data(fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61980596532a487cace1436b34d02761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "    \"embedding_dim\": [fasttext.vector_size]\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=500)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.231640</td>\n",
       "      <td>0.493279</td>\n",
       "      <td>0.917324</td>\n",
       "      <td>0.618234</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.234651</td>\n",
       "      <td>0.477880</td>\n",
       "      <td>0.926903</td>\n",
       "      <td>0.608981</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.240005</td>\n",
       "      <td>0.468102</td>\n",
       "      <td>0.938602</td>\n",
       "      <td>0.603304</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.251168</td>\n",
       "      <td>0.465856</td>\n",
       "      <td>0.938855</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.240357</td>\n",
       "      <td>0.502448</td>\n",
       "      <td>0.879451</td>\n",
       "      <td>0.617864</td>\n",
       "      <td>0.895268</td>\n",
       "      <td>0.404294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.228496</td>\n",
       "      <td>0.488206</td>\n",
       "      <td>0.951508</td>\n",
       "      <td>0.619465</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.237049</td>\n",
       "      <td>0.467153</td>\n",
       "      <td>0.951446</td>\n",
       "      <td>0.605947</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.240002</td>\n",
       "      <td>0.462152</td>\n",
       "      <td>0.953432</td>\n",
       "      <td>0.601858</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.467859</td>\n",
       "      <td>0.923907</td>\n",
       "      <td>0.599378</td>\n",
       "      <td>0.865254</td>\n",
       "      <td>0.388490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.254995</td>\n",
       "      <td>0.467859</td>\n",
       "      <td>0.923907</td>\n",
       "      <td>0.599378</td>\n",
       "      <td>0.865254</td>\n",
       "      <td>0.388490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.253651</td>\n",
       "      <td>0.456968</td>\n",
       "      <td>0.971029</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.239406</td>\n",
       "      <td>0.457065</td>\n",
       "      <td>0.965473</td>\n",
       "      <td>0.600378</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.238357</td>\n",
       "      <td>0.457065</td>\n",
       "      <td>0.965473</td>\n",
       "      <td>0.600378</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.258812</td>\n",
       "      <td>0.432296</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.584544</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.280286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.261906</td>\n",
       "      <td>0.432603</td>\n",
       "      <td>0.988716</td>\n",
       "      <td>0.584527</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.280286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.264557</td>\n",
       "      <td>0.432603</td>\n",
       "      <td>0.988716</td>\n",
       "      <td>0.584527</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.280286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.262849</td>\n",
       "      <td>0.425210</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.579649</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.262046</td>\n",
       "      <td>0.425210</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.579649</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  embedding_dim  val_loss  val_precision  \\\n",
       "12          200           5      0.0            300  0.231640       0.493279   \n",
       "8           150           5      0.5            300  0.234651       0.477880   \n",
       "9           150          10      0.0            300  0.240005       0.468102   \n",
       "17          200          10      0.5            300  0.251168       0.465856   \n",
       "4           100          10      0.3            300  0.240357       0.502448   \n",
       "2           100           5      0.5            300  0.228496       0.488206   \n",
       "11          150          10      0.5            300  0.237049       0.467153   \n",
       "14          200           5      0.5            300  0.240002       0.462152   \n",
       "5           100          10      0.5            300  0.252493       0.467859   \n",
       "3           100          10      0.0            300  0.254995       0.467859   \n",
       "7           150           5      0.3            300  0.253651       0.456968   \n",
       "6           150           5      0.0            300  0.239406       0.457065   \n",
       "13          200           5      0.3            300  0.238357       0.457065   \n",
       "1           100           5      0.3            300  0.258812       0.432296   \n",
       "16          200          10      0.3            300  0.261906       0.432603   \n",
       "0           100           5      0.0            300  0.264557       0.432603   \n",
       "10          150          10      0.3            300  0.262849       0.425210   \n",
       "15          200          10      0.0            300  0.262046       0.425210   \n",
       "\n",
       "    val_recall    val_f1  val_under  val_over  \n",
       "12    0.917324  0.618234   0.953125  0.422784  \n",
       "8     0.926903  0.608981   0.953125  0.422784  \n",
       "9     0.938602  0.603304   0.953125  0.422784  \n",
       "17    0.938855  0.601375   0.953125  0.422784  \n",
       "4     0.879451  0.617864   0.895268  0.404294  \n",
       "2     0.951508  0.619465   0.917540  0.375694  \n",
       "11    0.951446  0.605947   0.917540  0.375694  \n",
       "14    0.953432  0.601858   0.917540  0.375694  \n",
       "5     0.923907  0.599378   0.865254  0.388490  \n",
       "3     0.923907  0.599378   0.865254  0.388490  \n",
       "7     0.971029  0.601277   0.905265  0.342394  \n",
       "6     0.965473  0.600378   0.905265  0.342394  \n",
       "13    0.965473  0.600378   0.905265  0.342394  \n",
       "1     0.989978  0.584544   0.956955  0.280286  \n",
       "16    0.988716  0.584527   0.953125  0.280286  \n",
       "0     0.988716  0.584527   0.953125  0.280286  \n",
       "10    0.997376  0.579649   0.956955  0.243891  \n",
       "15    0.997376  0.579649   0.956955  0.243891  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under\"] + df[\"val_over\"] + df[\"val_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(fasttext.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:374: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "  rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13618898c3b14423adcdf21d061d5e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "model = BaselineModel(hidden_size=200, num_layers=5, dropout=0.0, embedding_dim=fasttext.vector_size)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/fasttext_lstm\",\n",
    "                     max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b16bcba0564c4482c1f74f34cdce50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_f1            0.5579043528480463\n",
      "        test_loss           0.22975552082061768\n",
      "        test_over           0.5705736238317893\n",
      "     test_precision         0.42399803102213496\n",
      "       test_recall          0.9242292002107206\n",
      "       test_under           0.9521838657630957\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# FORM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation.form import FORM\n",
    "from segmentation.metrics import pairwise_metrics, under_over_segmentation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list()\n",
    "\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    _, pred = np.unique(FORM(list(xi)), return_inverse=True)\n",
    "    _, target = np.unique(yi, return_inverse=True)\n",
    "    metrics.append((*pairwise_metrics(target, pred), *under_over_segmentation(target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, under, over = zip(*metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.5814049743223666\n",
      "recall 0.5133868699516293\n",
      "f1 0.5321519792560437\n",
      "under 0.5352970694536924\n",
      "over 0.6040644823138202\n"
     ]
    }
   ],
   "source": [
    "print(\"precision\", np.mean(precision))\n",
    "print(\"recall\", np.mean(recall))\n",
    "print(\"f1\", np.mean(f1))\n",
    "print(\"under\", np.mean(under))\n",
    "print(\"over\", np.mean(over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results\n",
    "\n",
    "| Model | Precision | Recall | F1 |\n",
    "|---|---|---|---|\n",
    "|FORM|0.58|0.51|0.53|\n",
    "|Word2Vec LSTM|0.43|0.91|0.57|\n",
    "|FastText LSTM|0.44|0.90|0.57|\n",
    "|harte2vec LSTM|0.47|0.82|0.58|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0794419de3274e91b3c03cf443b2d4dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09f7b06e42204e6db91b86620bbad832": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "181333071b524258879a7beb93a34553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d337fe9379d24d8ead564157b0830360",
      "placeholder": "​",
      "style": "IPY_MODEL_543c8f77319b496082bc0a6fcea81056",
      "value": " 78/300 [00:29&lt;01:24,  2.62it/s]"
     }
    },
    "543c8f77319b496082bc0a6fcea81056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "568a4ed4d64a491eb58a05ba3d791e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09f7b06e42204e6db91b86620bbad832",
      "placeholder": "​",
      "style": "IPY_MODEL_76bc31648e214ebd9ee52925924b0524",
      "value": " 26%"
     }
    },
    "76bc31648e214ebd9ee52925924b0524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77cbb088fb97462c8c296cb15081308c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "995bc64903664453a7bd467773db0541": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_568a4ed4d64a491eb58a05ba3d791e0c",
       "IPY_MODEL_b988fb2118f2471db06c239711537eb4",
       "IPY_MODEL_181333071b524258879a7beb93a34553"
      ],
      "layout": "IPY_MODEL_e5fdde8faaa6426eb8f271f22289633e"
     }
    },
    "b988fb2118f2471db06c239711537eb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77cbb088fb97462c8c296cb15081308c",
      "max": 300,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0794419de3274e91b3c03cf443b2d4dd",
      "value": 78
     }
    },
    "d337fe9379d24d8ead564157b0830360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5fdde8faaa6426eb8f271f22289633e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
