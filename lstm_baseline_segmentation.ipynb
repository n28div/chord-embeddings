{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sqn_Jmp7OQK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "er9gPVlMlF4n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"segmentation_dataset.npz\", \"rb\") as f:\n",
    "  npz = np.load(f, allow_pickle=True)\n",
    "  X_train, y_train, X_valid, y_valid, X_test, y_test = npz.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad all the sequences to the longest sequence in the split and encode them using the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = list(set.union(*[set(yi) for yi in y_train]))\n",
    "encoder = OneHotEncoder().fit(np.array(labels).reshape(-1, 1))\n",
    "\n",
    "def embed_data(model, X, y, encoder):\n",
    "    seq_length = [len(xi) for xi in X]\n",
    "    max_seq_length = max(seq_length)\n",
    "    \n",
    "    padded_seq = np.stack(\n",
    "        [np.pad(xi, (0, max_seq_length - len(xi)), constant_values=\"P\") \n",
    "         for xi in X])\n",
    "    \n",
    "    padding_mask = torch.tensor((padded_seq == \"P\").astype(int))\n",
    "    padded_seq[padding_mask == 1] = \"N\"\n",
    "    \n",
    "    embedded_seq = torch.tensor(np.stack([[model[sample]\n",
    "                                         for sample in seq] \n",
    "                                        for seq in padded_seq]))\n",
    "    \n",
    "    \n",
    "    encoded_labels = [torch.tensor(encoder.transform(yi.reshape(-1, 1)).todense()) for yi in y]\n",
    "    padded_labels = pad_sequence(encoded_labels, batch_first=True, padding_value=0)\n",
    "  \n",
    "    return embedded_seq, padded_labels, padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FORM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation.form import FORM\n",
    "from mir_eval.util import boundaries_to_intervals\n",
    "from mir_eval.segment import pairwise, nce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list()\n",
    "\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    intervals = boundaries_to_intervals(np.arange(len(yi) + 1))\n",
    "    _, pred = np.unique(FORM(list(xi)), return_inverse=True)\n",
    "    _, target = np.unique(yi, return_inverse=True)\n",
    "    \n",
    "    pairwise_score = pairwise(intervals, target, intervals, pred)\n",
    "    nce_score = pairwise(intervals, target, intervals, pred)\n",
    "\n",
    "    metrics.append((*pairwise_score, *nce_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, under, over, under_over_f = zip(*metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.579736595980633\n",
      "recall 0.5115957902021979\n",
      "f1 0.5303532083207078\n",
      "under 0.579736595980633\n",
      "over 0.5115957902021979\n",
      "under_over_f 0.5303532083207078\n"
     ]
    }
   ],
   "source": [
    "print(\"precision\", np.mean(precision))\n",
    "print(\"recall\", np.mean(recall))\n",
    "print(\"f1\", np.mean(f1))\n",
    "print(\"under\", np.mean(under))\n",
    "print(\"over\", np.mean(over))\n",
    "print(\"under_over_f\", np.mean(under_over_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwrXF12H8R5x",
    "tags": []
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XWizPjGUmLhG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from mir_eval.util import boundaries_to_intervals\n",
    "from mir_eval.segment import pairwise, nce\n",
    "from collections import defaultdict\n",
    "from more_itertools import stagger\n",
    "\n",
    "class BaselineModel(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 10,\n",
    "                 hidden_size: int = 100, \n",
    "                 dropout: float = 0.0,\n",
    "                 num_layers: int = 1, \n",
    "                 num_labels: int = 10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size,\n",
    "                            dropout=dropout,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.classification = nn.Linear(hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def _predict(self, batch):\n",
    "        x, y, mask = batch\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.classification(x)\n",
    "        x = self.softmax(x)\n",
    "               \n",
    "        loss = nn.functional.binary_cross_entropy(x[mask == 0].float(), y[mask == 0].float())\n",
    "        return x, loss\n",
    "    \n",
    "    def _test(self, batch):\n",
    "        metrics = defaultdict(list)\n",
    "        mask = batch[-1]\n",
    "        y = batch[-2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred, loss = self._predict(batch)\n",
    "            \n",
    "            for pi, yi, mi in zip(pred, y, mask):               \n",
    "                pi = pi[mi == 0].argmax(axis=-1).cpu().numpy()\n",
    "                _, pi = np.unique(pi, return_inverse=True)\n",
    "            \n",
    "                yi = yi[mi == 0].argmax(axis=-1).cpu().numpy()\n",
    "                _, yi = np.unique(yi, return_inverse=True)\n",
    "            \n",
    "                intervals = boundaries_to_intervals(np.arange(len(yi) + 1))\n",
    "                precision, recall, f1 = pairwise(intervals, yi, intervals, pi)\n",
    "                metrics[\"p_precision\"].append(precision)\n",
    "                metrics[\"p_recall\"].append(recall)\n",
    "                metrics[\"p_f1\"].append(f1)\n",
    "                over, under, under_over_f1 = nce(intervals, yi, intervals, pi)\n",
    "                metrics[\"under\"] = under\n",
    "                metrics[\"over\"] = over\n",
    "                metrics[\"under_over_f1\"] = under_over_f1\n",
    "        \n",
    "        metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "        return loss, metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss = self._predict(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, metrics = self._test(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        for k, m in metrics.items(): self.log(f\"val_{k}\", m)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, metrics = self._test(batch)        \n",
    "        self.log(\"test_loss\", loss)\n",
    "        for k, m in metrics.items(): self.log(f\"test_{k}\", m)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, valid = None, test = None, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self._train = train\n",
    "        self._valid = valid\n",
    "        self._test = test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(list(zip(*self._train)), batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self._valid is not None:\n",
    "            return DataLoader(list(zip(*self._valid)), batch_size=self.batch_size)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        if self._test is not None:\n",
    "            return DataLoader(list(zip(*self._test)), batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG50CVxK8cme",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train using harte2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_data(harte2vec, X_train, y_train, encoder), \n",
    "                  embed_data(harte2vec, X_valid, y_valid, encoder), \n",
    "                  embed_data(harte2vec, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "def train(config, data=data, epochs=500, validate=True, log=False):\n",
    "    model = BaselineModel(**config)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1,\n",
    "                         enable_progress_bar=False,\n",
    "                         callbacks=[\n",
    "                             EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                             StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "                         ])\n",
    "    trainer.fit(model, datamodule=data)\n",
    "    results = trainer.validate(model, datamodule=data, verbose=False) if validate else None\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902fa0842ed54d12be7590e4441395b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=350)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_p_precision</th>\n",
       "      <th>val_p_recall</th>\n",
       "      <th>val_p_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "      <th>val_under_over_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.235665</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.944230</td>\n",
       "      <td>0.601772</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>0.466584</td>\n",
       "      <td>0.942246</td>\n",
       "      <td>0.601650</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.242745</td>\n",
       "      <td>0.466810</td>\n",
       "      <td>0.931009</td>\n",
       "      <td>0.599565</td>\n",
       "      <td>0.895268</td>\n",
       "      <td>0.404294</td>\n",
       "      <td>0.557036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.242401</td>\n",
       "      <td>0.468012</td>\n",
       "      <td>0.922220</td>\n",
       "      <td>0.598847</td>\n",
       "      <td>0.895268</td>\n",
       "      <td>0.404294</td>\n",
       "      <td>0.557036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.233881</td>\n",
       "      <td>0.483942</td>\n",
       "      <td>0.930402</td>\n",
       "      <td>0.615055</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.236386</td>\n",
       "      <td>0.481569</td>\n",
       "      <td>0.936734</td>\n",
       "      <td>0.614449</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241976</td>\n",
       "      <td>0.467688</td>\n",
       "      <td>0.923650</td>\n",
       "      <td>0.597678</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.364232</td>\n",
       "      <td>0.522656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250266</td>\n",
       "      <td>0.461019</td>\n",
       "      <td>0.954597</td>\n",
       "      <td>0.601122</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.362908</td>\n",
       "      <td>0.517337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.263864</td>\n",
       "      <td>0.461467</td>\n",
       "      <td>0.937786</td>\n",
       "      <td>0.596909</td>\n",
       "      <td>0.847409</td>\n",
       "      <td>0.363408</td>\n",
       "      <td>0.508673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.237825</td>\n",
       "      <td>0.456414</td>\n",
       "      <td>0.959541</td>\n",
       "      <td>0.596450</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.497789</td>\n",
       "      <td>0.894892</td>\n",
       "      <td>0.615352</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.244866</td>\n",
       "      <td>0.482127</td>\n",
       "      <td>0.908574</td>\n",
       "      <td>0.608365</td>\n",
       "      <td>0.937257</td>\n",
       "      <td>0.305693</td>\n",
       "      <td>0.461021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236067</td>\n",
       "      <td>0.475317</td>\n",
       "      <td>0.906448</td>\n",
       "      <td>0.601680</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.234552</td>\n",
       "      <td>0.464233</td>\n",
       "      <td>0.942827</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.233717</td>\n",
       "      <td>0.487428</td>\n",
       "      <td>0.926886</td>\n",
       "      <td>0.615633</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.292234</td>\n",
       "      <td>0.443284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257022</td>\n",
       "      <td>0.439760</td>\n",
       "      <td>0.980886</td>\n",
       "      <td>0.589040</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.292234</td>\n",
       "      <td>0.443284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263157</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  val_loss  val_p_precision  val_p_recall  \\\n",
       "14          200           5      0.5  0.235665         0.465022      0.944230   \n",
       "12          200           5      0.0  0.242081         0.466584      0.942246   \n",
       "17          200          10      0.5  0.242745         0.466810      0.931009   \n",
       "5           100          10      0.5  0.242401         0.468012      0.922220   \n",
       "10          150          10      0.3  0.233881         0.483942      0.930402   \n",
       "16          200          10      0.3  0.236386         0.481569      0.936734   \n",
       "6           150           5      0.0  0.241976         0.467688      0.923650   \n",
       "3           100          10      0.0  0.250266         0.461019      0.954597   \n",
       "11          150          10      0.5  0.263864         0.461467      0.937786   \n",
       "2           100           5      0.5  0.237825         0.456414      0.959541   \n",
       "1           100           5      0.3  0.238960         0.497789      0.894892   \n",
       "13          200           5      0.3  0.244866         0.482127      0.908574   \n",
       "0           100           5      0.0  0.236067         0.475317      0.906448   \n",
       "8           150           5      0.5  0.234552         0.464233      0.942827   \n",
       "7           150           5      0.3  0.233717         0.487428      0.926886   \n",
       "15          200          10      0.0  0.257022         0.439760      0.980886   \n",
       "4           100          10      0.3  0.261958         0.414858      1.000000   \n",
       "9           150          10      0.0  0.263157         0.414858      1.000000   \n",
       "\n",
       "    val_p_f1  val_under  val_over  val_under_over_f1  \n",
       "14  0.601772   0.953125  0.422784           0.585745  \n",
       "12  0.601650   0.953125  0.422784           0.585745  \n",
       "17  0.599565   0.895268  0.404294           0.557036  \n",
       "5   0.598847   0.895268  0.404294           0.557036  \n",
       "10  0.615055   0.917540  0.375694           0.533105  \n",
       "16  0.614449   0.917540  0.375694           0.533105  \n",
       "6   0.597678   0.924982  0.364232           0.522656  \n",
       "3   0.601122   0.900554  0.362908           0.517337  \n",
       "11  0.596909   0.847409  0.363408           0.508673  \n",
       "2   0.596450   0.905265  0.342394           0.496862  \n",
       "1   0.615352   0.905265  0.314855           0.467212  \n",
       "13  0.608365   0.937257  0.305693           0.461021  \n",
       "0   0.601680   0.905265  0.314855           0.467212  \n",
       "8   0.601277   0.905265  0.314855           0.467212  \n",
       "7   0.615633   0.917540  0.292234           0.443284  \n",
       "15  0.589040   0.917540  0.292234           0.443284  \n",
       "4   0.570534   0.000000  0.212421           0.000000  \n",
       "9   0.570534   0.000000  0.212421           0.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under_over_f1\"] + df[\"val_p_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_data(harte2vec, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(harte2vec, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f4e6eadfd941cf90cea473d91c2079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BaselineModel(hidden_size=200, num_layers=5, dropout=0.3)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/harte2vec_lstm\",\n",
    "                     max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", patience=4),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9274e8d36240d5aa19e8d08deec85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.22617462277412415\n",
      "        test_over           0.9521838657630957\n",
      "        test_p_f1           0.5802971410576294\n",
      "    test_p_precision        0.45471043918153536\n",
      "      test_p_recall          0.871113989824047\n",
      "       test_under           0.5705736238317893\n",
      "   test_under_over_f1       0.7135620774219917\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(word2vec.wv, X_train, y_train, encoder), \n",
    "                  embed_data(word2vec.wv, X_valid, y_valid, encoder), \n",
    "                  embed_data(word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf5889f868e4959bf770a9c8c2970ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "    \"embedding_dim\": [word2vec.vector_size]\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=500)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_p_precision</th>\n",
       "      <th>val_p_recall</th>\n",
       "      <th>val_p_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "      <th>val_under_over_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.229496</td>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.901733</td>\n",
       "      <td>0.627525</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.233097</td>\n",
       "      <td>0.485412</td>\n",
       "      <td>0.938468</td>\n",
       "      <td>0.614444</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.234857</td>\n",
       "      <td>0.468116</td>\n",
       "      <td>0.938438</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.277238</td>\n",
       "      <td>0.465132</td>\n",
       "      <td>0.938651</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.241033</td>\n",
       "      <td>0.465132</td>\n",
       "      <td>0.938651</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.237404</td>\n",
       "      <td>0.465132</td>\n",
       "      <td>0.938651</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.237029</td>\n",
       "      <td>0.468178</td>\n",
       "      <td>0.939640</td>\n",
       "      <td>0.599266</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.239311</td>\n",
       "      <td>0.518090</td>\n",
       "      <td>0.749159</td>\n",
       "      <td>0.590126</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.228320</td>\n",
       "      <td>0.488055</td>\n",
       "      <td>0.919191</td>\n",
       "      <td>0.614446</td>\n",
       "      <td>0.937257</td>\n",
       "      <td>0.399985</td>\n",
       "      <td>0.560689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.232632</td>\n",
       "      <td>0.467021</td>\n",
       "      <td>0.945209</td>\n",
       "      <td>0.602025</td>\n",
       "      <td>0.895268</td>\n",
       "      <td>0.404294</td>\n",
       "      <td>0.557036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.236795</td>\n",
       "      <td>0.511688</td>\n",
       "      <td>0.840740</td>\n",
       "      <td>0.615203</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.239063</td>\n",
       "      <td>0.518536</td>\n",
       "      <td>0.802121</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.230456</td>\n",
       "      <td>0.477183</td>\n",
       "      <td>0.939946</td>\n",
       "      <td>0.606322</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.469890</td>\n",
       "      <td>0.920703</td>\n",
       "      <td>0.599964</td>\n",
       "      <td>0.865254</td>\n",
       "      <td>0.388490</td>\n",
       "      <td>0.536222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.254178</td>\n",
       "      <td>0.475059</td>\n",
       "      <td>0.888153</td>\n",
       "      <td>0.595628</td>\n",
       "      <td>0.828497</td>\n",
       "      <td>0.396198</td>\n",
       "      <td>0.536050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.234094</td>\n",
       "      <td>0.503231</td>\n",
       "      <td>0.813659</td>\n",
       "      <td>0.600180</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.266652</td>\n",
       "      <td>0.431928</td>\n",
       "      <td>0.989100</td>\n",
       "      <td>0.583899</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.280286</td>\n",
       "      <td>0.433579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.257874</td>\n",
       "      <td>0.424148</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.578589</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  embedding_dim  val_loss  \\\n",
       "6           150           5      0.0            300  0.229496   \n",
       "7           150           5      0.3            300  0.233097   \n",
       "5           100          10      0.5            300  0.234857   \n",
       "17          200          10      0.5            300  0.277238   \n",
       "16          200          10      0.3            300  0.241033   \n",
       "15          200          10      0.0            300  0.237404   \n",
       "14          200           5      0.5            300  0.237029   \n",
       "3           100          10      0.0            300  0.239311   \n",
       "12          200           5      0.0            300  0.228320   \n",
       "1           100           5      0.3            300  0.232632   \n",
       "11          150          10      0.5            300  0.236795   \n",
       "9           150          10      0.0            300  0.239063   \n",
       "2           100           5      0.5            300  0.230456   \n",
       "4           100          10      0.3            300  0.250994   \n",
       "10          150          10      0.3            300  0.254178   \n",
       "0           100           5      0.0            300  0.234094   \n",
       "13          200           5      0.3            300  0.266652   \n",
       "8           150           5      0.5            300  0.257874   \n",
       "\n",
       "    val_p_precision  val_p_recall  val_p_f1  val_under  val_over  \\\n",
       "6          0.509183      0.901733  0.627525   0.953125  0.422784   \n",
       "7          0.485412      0.938468  0.614444   0.953125  0.422784   \n",
       "5          0.468116      0.938438  0.603175   0.953125  0.422784   \n",
       "17         0.465132      0.938651  0.600609   0.953125  0.422784   \n",
       "16         0.465132      0.938651  0.600609   0.953125  0.422784   \n",
       "15         0.465132      0.938651  0.600609   0.953125  0.422784   \n",
       "14         0.468178      0.939640  0.599266   0.953125  0.422784   \n",
       "3          0.518090      0.749159  0.590126   0.953125  0.422784   \n",
       "12         0.488055      0.919191  0.614446   0.937257  0.399985   \n",
       "1          0.467021      0.945209  0.602025   0.895268  0.404294   \n",
       "11         0.511688      0.840740  0.615203   0.917540  0.375694   \n",
       "9          0.518536      0.802121  0.607925   0.917540  0.375694   \n",
       "2          0.477183      0.939946  0.606322   0.917540  0.375694   \n",
       "4          0.469890      0.920703  0.599964   0.865254  0.388490   \n",
       "10         0.475059      0.888153  0.595628   0.828497  0.396198   \n",
       "0          0.503231      0.813659  0.600180   0.905265  0.342394   \n",
       "13         0.431928      0.989100  0.583899   0.956955  0.280286   \n",
       "8          0.424148      0.997363  0.578589   0.956955  0.243891   \n",
       "\n",
       "    val_under_over_f1  \n",
       "6            0.585745  \n",
       "7            0.585745  \n",
       "5            0.585745  \n",
       "17           0.585745  \n",
       "16           0.585745  \n",
       "15           0.585745  \n",
       "14           0.585745  \n",
       "3            0.585745  \n",
       "12           0.560689  \n",
       "1            0.557036  \n",
       "11           0.533105  \n",
       "9            0.533105  \n",
       "2            0.533105  \n",
       "4            0.536222  \n",
       "10           0.536050  \n",
       "0            0.496862  \n",
       "13           0.433579  \n",
       "8            0.388713  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under_over_f1\"] + df[\"val_p_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(word2vec.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:374: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "  rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4643cb77157d48c6af85c815a2cfb8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "model = BaselineModel(hidden_size=150, num_layers=5, dropout=0.0, embedding_dim=word2vec.vector_size)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/word2vec_lstm\",\n",
    "                     max_epochs=500, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83296de4abaa474da2a9c6bd7b9c78cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.23724155128002167\n",
      "        test_over           0.9390096744367639\n",
      "        test_p_f1           0.5591144111471511\n",
      "    test_p_precision        0.41818769077290363\n",
      "      test_p_recall         0.9402929224057823\n",
      "       test_under            0.542917723646533\n",
      "   test_under_over_f1       0.6880296505573138\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(fasttext.wv, X_train, y_train, encoder), \n",
    "                  embed_data(fasttext.wv, X_valid, y_valid, encoder), \n",
    "                  embed_data(fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31774e46c8e4413bee1aed7e0fb7e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "    \"embedding_dim\": [fasttext.vector_size]\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=500)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_p_precision</th>\n",
       "      <th>val_p_recall</th>\n",
       "      <th>val_p_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "      <th>val_under_over_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235180</td>\n",
       "      <td>0.472315</td>\n",
       "      <td>0.932854</td>\n",
       "      <td>0.605675</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.230125</td>\n",
       "      <td>0.487791</td>\n",
       "      <td>0.931132</td>\n",
       "      <td>0.617591</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.236430</td>\n",
       "      <td>0.501884</td>\n",
       "      <td>0.873821</td>\n",
       "      <td>0.616498</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235453</td>\n",
       "      <td>0.462677</td>\n",
       "      <td>0.955763</td>\n",
       "      <td>0.602737</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.240601</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.601108</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.601108</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.601108</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.235809</td>\n",
       "      <td>0.469069</td>\n",
       "      <td>0.928306</td>\n",
       "      <td>0.599905</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.364232</td>\n",
       "      <td>0.522656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.229235</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.955481</td>\n",
       "      <td>0.625609</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.234819</td>\n",
       "      <td>0.494367</td>\n",
       "      <td>0.905913</td>\n",
       "      <td>0.618927</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.238820</td>\n",
       "      <td>0.501178</td>\n",
       "      <td>0.901287</td>\n",
       "      <td>0.622020</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.333394</td>\n",
       "      <td>0.490129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>0.953540</td>\n",
       "      <td>0.607931</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.239498</td>\n",
       "      <td>0.457443</td>\n",
       "      <td>0.972995</td>\n",
       "      <td>0.601675</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>0.331562</td>\n",
       "      <td>0.481815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.253581</td>\n",
       "      <td>0.447586</td>\n",
       "      <td>0.973565</td>\n",
       "      <td>0.594164</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.454769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.252620</td>\n",
       "      <td>0.447586</td>\n",
       "      <td>0.973565</td>\n",
       "      <td>0.594164</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.454769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.263075</td>\n",
       "      <td>0.431928</td>\n",
       "      <td>0.989100</td>\n",
       "      <td>0.583899</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.280286</td>\n",
       "      <td>0.433579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.266852</td>\n",
       "      <td>0.421712</td>\n",
       "      <td>0.990880</td>\n",
       "      <td>0.575053</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.257009</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  embedding_dim  val_loss  \\\n",
       "8           150           5      0.5            300  0.235180   \n",
       "9           150          10      0.0            300  0.230125   \n",
       "11          150          10      0.5            300  0.236430   \n",
       "7           150           5      0.3            300  0.235453   \n",
       "13          200           5      0.3            300  0.240601   \n",
       "5           100          10      0.5            300  0.235676   \n",
       "10          150          10      0.3            300  0.238763   \n",
       "14          200           5      0.5            300  0.235809   \n",
       "2           100           5      0.5            300  0.229235   \n",
       "12          200           5      0.0            300  0.234819   \n",
       "6           150           5      0.0            300  0.238820   \n",
       "17          200          10      0.5            300  0.234899   \n",
       "1           100           5      0.3            300  0.239498   \n",
       "4           100          10      0.3            300  0.253581   \n",
       "0           100           5      0.0            300  0.252620   \n",
       "15          200          10      0.0            300  0.263075   \n",
       "16          200          10      0.3            300  0.266852   \n",
       "3           100          10      0.0            300  0.257009   \n",
       "\n",
       "    val_p_precision  val_p_recall  val_p_f1  val_under  val_over  \\\n",
       "8          0.472315      0.932854  0.605675   0.953125  0.422784   \n",
       "9          0.487791      0.931132  0.617591   0.917540  0.375694   \n",
       "11         0.501884      0.873821  0.616498   0.917540  0.375694   \n",
       "7          0.462677      0.955763  0.602737   0.917540  0.375694   \n",
       "13         0.461440      0.953274  0.601108   0.917540  0.375694   \n",
       "5          0.461440      0.953274  0.601108   0.917540  0.375694   \n",
       "10         0.461440      0.953274  0.601108   0.917540  0.375694   \n",
       "14         0.469069      0.928306  0.599905   0.924982  0.364232   \n",
       "2          0.494253      0.955481  0.625609   0.905265  0.342394   \n",
       "12         0.494367      0.905913  0.618927   0.905265  0.342394   \n",
       "6          0.501178      0.901287  0.622020   0.924982  0.333394   \n",
       "17         0.468707      0.953540  0.607931   0.905265  0.342394   \n",
       "1          0.457443      0.972995  0.601675   0.881099  0.331562   \n",
       "4          0.447586      0.973565  0.594164   0.881099  0.306476   \n",
       "0          0.447586      0.973565  0.594164   0.881099  0.306476   \n",
       "15         0.431928      0.989100  0.583899   0.956955  0.280286   \n",
       "16         0.421712      0.990880  0.575053   0.956955  0.243891   \n",
       "3          0.414858      1.000000  0.570534   0.000000  0.212421   \n",
       "\n",
       "    val_under_over_f1  \n",
       "8            0.585745  \n",
       "9            0.533105  \n",
       "11           0.533105  \n",
       "7            0.533105  \n",
       "13           0.533105  \n",
       "5            0.533105  \n",
       "10           0.533105  \n",
       "14           0.522656  \n",
       "2            0.496862  \n",
       "12           0.496862  \n",
       "6            0.490129  \n",
       "17           0.496862  \n",
       "1            0.481815  \n",
       "4            0.454769  \n",
       "0            0.454769  \n",
       "15           0.433579  \n",
       "16           0.388713  \n",
       "3            0.000000  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under_over_f1\"] + df[\"val_p_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "\n",
    "data = DataModule(embed_data(fasttext.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_data(fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a726c81ac4ca46938e0709c19ad34f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "model = BaselineModel(hidden_size=150, num_layers=5, dropout=0.5, embedding_dim=fasttext.vector_size)\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"models/fasttext_lstm\",\n",
    "                     max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "                     ])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57d793ce38945499ed46255c2ea22da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.22611752152442932\n",
      "        test_over           0.9390096744367639\n",
      "        test_p_f1           0.5647761606753763\n",
      "    test_p_precision        0.43267468200017056\n",
      "      test_p_recall         0.8948423975101675\n",
      "       test_under            0.542917723646533\n",
      "   test_under_over_f1       0.6880296505573138\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined embedding LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XWizPjGUmLhG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from mir_eval.util import boundaries_to_intervals\n",
    "from mir_eval.segment import pairwise, nce\n",
    "from collections import defaultdict\n",
    "from more_itertools import stagger\n",
    "\n",
    "class EmbeddingCombinedModel(BaselineModel):\n",
    "    def __init__(self,\n",
    "                 syntactic_embedding_dim: int = 300,\n",
    "                 semantic_embedding_dim: int = 10,\n",
    "                 hidden_size: int = 150, \n",
    "                 dropout: float = 0.5,\n",
    "                 num_layers: int = 5, \n",
    "                 num_labels: int = 10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.lstm = nn.LSTM(syntactic_embedding_dim + semantic_embedding_dim,\n",
    "                            hidden_size,\n",
    "                            dropout=dropout,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.classification = nn.Linear(hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def _predict(self, batch):\n",
    "        x_sem, x_syn, y, mask = batch\n",
    "        x = torch.cat((x_sem, x_syn), 2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.classification(x)\n",
    "        x = self.softmax(x)\n",
    "               \n",
    "        loss = nn.functional.binary_cross_entropy(x[mask == 0].float(), y[mask == 0].float())\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "def train(config, data, epochs=500, validate=True, log=False):\n",
    "    model = EmbeddingCombinedModel(**config)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1,\n",
    "                         enable_progress_bar=False,\n",
    "                         callbacks=[\n",
    "                             EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=2),\n",
    "                             StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "                         ])\n",
    "    trainer.fit(model, datamodule=data)\n",
    "    results = trainer.validate(model, datamodule=data, verbose=False) if validate else None\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fused_data(sem_model, syn_model, X, y, encoder):\n",
    "    sem, y_out, mask = embed_data(sem_model, X, y, encoder)\n",
    "    syn, _, _ = embed_data(syn_model, X, y, encoder)\n",
    "    return sem, syn, y_out, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## harte2vec + word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparamters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_fused_data(harte2vec, word2vec.wv, X_train, y_train, encoder), \n",
    "                  embed_fused_data(harte2vec, word2vec.wv, X_valid, y_valid, encoder), \n",
    "                  embed_fused_data(harte2vec, word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974e073fbcb246479a7a7c9ea273d03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=350)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_p_precision</th>\n",
       "      <th>val_p_recall</th>\n",
       "      <th>val_p_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "      <th>val_under_over_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232016</td>\n",
       "      <td>0.479754</td>\n",
       "      <td>0.929334</td>\n",
       "      <td>0.610204</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.242138</td>\n",
       "      <td>0.464726</td>\n",
       "      <td>0.941075</td>\n",
       "      <td>0.600773</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.229245</td>\n",
       "      <td>0.506195</td>\n",
       "      <td>0.849329</td>\n",
       "      <td>0.610125</td>\n",
       "      <td>0.399985</td>\n",
       "      <td>0.937257</td>\n",
       "      <td>0.560689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.235493</td>\n",
       "      <td>0.510196</td>\n",
       "      <td>0.846244</td>\n",
       "      <td>0.615666</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.226737</td>\n",
       "      <td>0.503681</td>\n",
       "      <td>0.854756</td>\n",
       "      <td>0.613035</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233846</td>\n",
       "      <td>0.471798</td>\n",
       "      <td>0.960169</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.229955</td>\n",
       "      <td>0.493245</td>\n",
       "      <td>0.929389</td>\n",
       "      <td>0.621244</td>\n",
       "      <td>0.364232</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.522656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.230569</td>\n",
       "      <td>0.504955</td>\n",
       "      <td>0.893656</td>\n",
       "      <td>0.620260</td>\n",
       "      <td>0.364232</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.522656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.243866</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.601108</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231551</td>\n",
       "      <td>0.491164</td>\n",
       "      <td>0.917524</td>\n",
       "      <td>0.613335</td>\n",
       "      <td>0.333394</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.490129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231155</td>\n",
       "      <td>0.477820</td>\n",
       "      <td>0.908824</td>\n",
       "      <td>0.608484</td>\n",
       "      <td>0.333394</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.490129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.236370</td>\n",
       "      <td>0.467830</td>\n",
       "      <td>0.909365</td>\n",
       "      <td>0.587564</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230334</td>\n",
       "      <td>0.495807</td>\n",
       "      <td>0.859839</td>\n",
       "      <td>0.603326</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.258604</td>\n",
       "      <td>0.447710</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.594041</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.234593</td>\n",
       "      <td>0.484366</td>\n",
       "      <td>0.897725</td>\n",
       "      <td>0.603027</td>\n",
       "      <td>0.292234</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.443284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.424525</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.578919</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.262811</td>\n",
       "      <td>0.424525</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.578919</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.423066</td>\n",
       "      <td>0.997417</td>\n",
       "      <td>0.577476</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  val_loss  val_p_precision  val_p_recall  \\\n",
       "3           100          10      0.0  0.232016         0.479754      0.929334   \n",
       "4           100          10      0.3  0.242138         0.464726      0.941075   \n",
       "1           100           5      0.3  0.229245         0.506195      0.849329   \n",
       "10          150          10      0.3  0.235493         0.510196      0.846244   \n",
       "7           150           5      0.3  0.226737         0.503681      0.854756   \n",
       "15          200          10      0.0  0.233846         0.471798      0.960169   \n",
       "14          200           5      0.5  0.229955         0.493245      0.929389   \n",
       "13          200           5      0.3  0.230569         0.504955      0.893656   \n",
       "17          200          10      0.5  0.243866         0.461440      0.953274   \n",
       "12          200           5      0.0  0.231551         0.491164      0.917524   \n",
       "6           150           5      0.0  0.231155         0.477820      0.908824   \n",
       "8           150           5      0.5  0.236370         0.467830      0.909365   \n",
       "0           100           5      0.0  0.230334         0.495807      0.859839   \n",
       "5           100          10      0.5  0.258604         0.447710      0.972603   \n",
       "2           100           5      0.5  0.234593         0.484366      0.897725   \n",
       "11          150          10      0.5  0.260879         0.424525      0.997363   \n",
       "16          200          10      0.3  0.262811         0.424525      0.997363   \n",
       "9           150          10      0.0  0.265170         0.423066      0.997417   \n",
       "\n",
       "    val_p_f1  val_under  val_over  val_under_over_f1  \n",
       "3   0.610204   0.422784  0.953125           0.585745  \n",
       "4   0.600773   0.422784  0.953125           0.585745  \n",
       "1   0.610125   0.399985  0.937257           0.560689  \n",
       "10  0.615666   0.375694  0.917540           0.533105  \n",
       "7   0.613035   0.375694  0.917540           0.533105  \n",
       "15  0.612197   0.375694  0.917540           0.533105  \n",
       "14  0.621244   0.364232  0.924982           0.522656  \n",
       "13  0.620260   0.364232  0.924982           0.522656  \n",
       "17  0.601108   0.375694  0.917540           0.533105  \n",
       "12  0.613335   0.333394  0.924982           0.490129  \n",
       "6   0.608484   0.333394  0.924982           0.490129  \n",
       "8   0.587564   0.342394  0.905265           0.496862  \n",
       "0   0.603326   0.314855  0.905265           0.467212  \n",
       "5   0.594041   0.314855  0.905265           0.467212  \n",
       "2   0.603027   0.292234  0.917540           0.443284  \n",
       "11  0.578919   0.243891  0.956955           0.388713  \n",
       "16  0.578919   0.243891  0.956955           0.388713  \n",
       "9   0.577476   0.243891  0.956955           0.388713  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under_over_f1\"] + df[\"val_p_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"word2vec.gensim\")\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_fused_data(harte2vec, word2vec.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_fused_data(harte2vec, word2vec.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c3313883364728be2903d70ad82a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "\n",
    "model = EmbeddingCombinedModel(hidden_size=100, dropout=0.2, num_layers=10)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", patience=3),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2)])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a951eaf68d14a4fb9d95dd97bfd9588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.22827650606632233\n",
      "        test_over           0.9296958362175696\n",
      "        test_p_f1           0.5852919702275513\n",
      "    test_p_precision        0.47065793888432794\n",
      "      test_p_recall         0.8383740043904989\n",
      "       test_under           0.5890391885847058\n",
      "   test_under_over_f1       0.7211623779697482\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.22827650606632233,\n",
       "  'test_p_precision': 0.47065793888432794,\n",
       "  'test_p_recall': 0.8383740043904989,\n",
       "  'test_p_f1': 0.5852919702275513,\n",
       "  'test_under': 0.5890391885847058,\n",
       "  'test_over': 0.9296958362175696,\n",
       "  'test_under_over_f1': 0.7211623779697482}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## harte2vec + fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparamters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_fused_data(harte2vec, fasttext.wv, X_train, y_train, encoder), \n",
    "                  embed_fused_data(harte2vec, fasttext.wv, X_valid, y_valid, encoder), \n",
    "                  embed_fused_data(harte2vec, fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe052882d66f459ea99b5e4f2028ef10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/nicolas/harte2vec/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parameters = { \n",
    "    \"hidden_size\": [100, 150, 200],\n",
    "    \"num_layers\": [5, 10],\n",
    "    \"dropout\": [0.0, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "results = list()\n",
    "combinations = list(enumerate(product(*parameters.values())))\n",
    "\n",
    "for idx, params in tqdm(combinations):\n",
    "    params = dict(zip(parameters.keys(), params))\n",
    "    res, _ = train(params, data=data, epochs=350)\n",
    "    results.append((params, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_p_precision</th>\n",
       "      <th>val_p_recall</th>\n",
       "      <th>val_p_f1</th>\n",
       "      <th>val_under</th>\n",
       "      <th>val_over</th>\n",
       "      <th>val_under_over_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233452</td>\n",
       "      <td>0.484778</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>0.613704</td>\n",
       "      <td>0.422784</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.585745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.233735</td>\n",
       "      <td>0.513454</td>\n",
       "      <td>0.875120</td>\n",
       "      <td>0.625826</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230136</td>\n",
       "      <td>0.500856</td>\n",
       "      <td>0.918713</td>\n",
       "      <td>0.623140</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.236662</td>\n",
       "      <td>0.500447</td>\n",
       "      <td>0.883234</td>\n",
       "      <td>0.617806</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230367</td>\n",
       "      <td>0.500901</td>\n",
       "      <td>0.882694</td>\n",
       "      <td>0.614961</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.238848</td>\n",
       "      <td>0.518536</td>\n",
       "      <td>0.802121</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.234945</td>\n",
       "      <td>0.507823</td>\n",
       "      <td>0.870278</td>\n",
       "      <td>0.616860</td>\n",
       "      <td>0.364232</td>\n",
       "      <td>0.924982</td>\n",
       "      <td>0.522656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.231546</td>\n",
       "      <td>0.470326</td>\n",
       "      <td>0.931868</td>\n",
       "      <td>0.604009</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.242253</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.953274</td>\n",
       "      <td>0.601108</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.533105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.484155</td>\n",
       "      <td>0.898996</td>\n",
       "      <td>0.605798</td>\n",
       "      <td>0.342394</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287712</td>\n",
       "      <td>0.473690</td>\n",
       "      <td>0.894595</td>\n",
       "      <td>0.596421</td>\n",
       "      <td>0.378097</td>\n",
       "      <td>0.759424</td>\n",
       "      <td>0.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.234888</td>\n",
       "      <td>0.504309</td>\n",
       "      <td>0.938118</td>\n",
       "      <td>0.630823</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.237393</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.775832</td>\n",
       "      <td>0.583764</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.467212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.260444</td>\n",
       "      <td>0.434603</td>\n",
       "      <td>0.988456</td>\n",
       "      <td>0.585808</td>\n",
       "      <td>0.283856</td>\n",
       "      <td>0.905265</td>\n",
       "      <td>0.432193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262684</td>\n",
       "      <td>0.431996</td>\n",
       "      <td>0.989909</td>\n",
       "      <td>0.584150</td>\n",
       "      <td>0.280286</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.433579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.264141</td>\n",
       "      <td>0.423066</td>\n",
       "      <td>0.997417</td>\n",
       "      <td>0.577476</td>\n",
       "      <td>0.243891</td>\n",
       "      <td>0.956955</td>\n",
       "      <td>0.388713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.261856</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570534</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263292</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570534</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout  val_loss  val_p_precision  val_p_recall  \\\n",
       "0           100           5      0.0  0.233452         0.484778      0.924081   \n",
       "8           150           5      0.5  0.233735         0.513454      0.875120   \n",
       "6           150           5      0.0  0.230136         0.500856      0.918713   \n",
       "16          200          10      0.3  0.236662         0.500447      0.883234   \n",
       "12          200           5      0.0  0.230367         0.500901      0.882694   \n",
       "5           100          10      0.5  0.238848         0.518536      0.802121   \n",
       "7           150           5      0.3  0.234945         0.507823      0.870278   \n",
       "11          150          10      0.5  0.231546         0.470326      0.931868   \n",
       "17          200          10      0.5  0.242253         0.461440      0.953274   \n",
       "2           100           5      0.5  0.231049         0.484155      0.898996   \n",
       "3           100          10      0.0  0.287712         0.473690      0.894595   \n",
       "1           100           5      0.3  0.234888         0.504309      0.938118   \n",
       "13          200           5      0.3  0.237393         0.497036      0.775832   \n",
       "10          150          10      0.3  0.260444         0.434603      0.988456   \n",
       "9           150          10      0.0  0.262684         0.431996      0.989909   \n",
       "14          200           5      0.5  0.264141         0.423066      0.997417   \n",
       "4           100          10      0.3  0.261856         0.414858      1.000000   \n",
       "15          200          10      0.0  0.263292         0.414858      1.000000   \n",
       "\n",
       "    val_p_f1  val_under  val_over  val_under_over_f1  \n",
       "0   0.613704   0.422784  0.953125           0.585745  \n",
       "8   0.625826   0.375694  0.917540           0.533105  \n",
       "6   0.623140   0.375694  0.917540           0.533105  \n",
       "16  0.617806   0.375694  0.917540           0.533105  \n",
       "12  0.614961   0.375694  0.917540           0.533105  \n",
       "5   0.607925   0.375694  0.917540           0.533105  \n",
       "7   0.616860   0.364232  0.924982           0.522656  \n",
       "11  0.604009   0.375694  0.917540           0.533105  \n",
       "17  0.601108   0.375694  0.917540           0.533105  \n",
       "2   0.605798   0.342394  0.905265           0.496862  \n",
       "3   0.596421   0.378097  0.759424           0.504845  \n",
       "1   0.630823   0.314855  0.905265           0.467212  \n",
       "13  0.583764   0.314855  0.905265           0.467212  \n",
       "10  0.585808   0.283856  0.905265           0.432193  \n",
       "9   0.584150   0.280286  0.956955           0.433579  \n",
       "14  0.577476   0.243891  0.956955           0.388713  \n",
       "4   0.570534   0.212421  0.000000           0.000000  \n",
       "15  0.570534   0.212421  0.000000           0.000000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict([dict(**res[0], **res[1][0]) for res in results])\n",
    "df.iloc[(df[\"val_under_over_f1\"] + df[\"val_p_f1\"]).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harte2vec.harte2vec import Harte2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "fasttext = FastText.load(\"fasttext.gensim\")\n",
    "harte2vec = Harte2Vec.from_pretrained(\"harte2vec.pt\")\n",
    "\n",
    "data = DataModule(embed_fused_data(harte2vec, fasttext.wv, np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), encoder),\n",
    "                  test=embed_fused_data(harte2vec, fasttext.wv, X_test, y_test, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6b1dc3cb094f428cb6c48584bb6b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
    "import logging\n",
    "\n",
    "\n",
    "model = EmbeddingCombinedModel(hidden_size=100, dropout=0.0, num_layers=5)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=350, accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[\n",
    "                         EarlyStopping(monitor=\"train_loss\", patience=3),\n",
    "                         StochasticWeightAveraging(swa_lrs=1e-2)])\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e496f3ee05ca43e4a06d43d095f156fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.2311086803674698\n",
      "        test_over           0.9808995550235486\n",
      "        test_p_f1           0.5855610905561515\n",
      "    test_p_precision        0.4692533990966828\n",
      "      test_p_recall         0.8374025328635752\n",
      "       test_under           0.6076136569506746\n",
      "   test_under_over_f1       0.7503972409375457\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2311086803674698,\n",
       "  'test_p_precision': 0.4692533990966828,\n",
       "  'test_p_recall': 0.8374025328635752,\n",
       "  'test_p_f1': 0.5855610905561515,\n",
       "  'test_under': 0.6076136569506746,\n",
       "  'test_over': 0.9808995550235486,\n",
       "  'test_under_over_f1': 0.7503972409375457}]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "1a065321f2d93fd20f60a317fd78c7406ef3cd4260c4249e728d2806e6b26b96"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0794419de3274e91b3c03cf443b2d4dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09f7b06e42204e6db91b86620bbad832": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "181333071b524258879a7beb93a34553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d337fe9379d24d8ead564157b0830360",
      "placeholder": "​",
      "style": "IPY_MODEL_543c8f77319b496082bc0a6fcea81056",
      "value": " 78/300 [00:29&lt;01:24,  2.62it/s]"
     }
    },
    "543c8f77319b496082bc0a6fcea81056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "568a4ed4d64a491eb58a05ba3d791e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09f7b06e42204e6db91b86620bbad832",
      "placeholder": "​",
      "style": "IPY_MODEL_76bc31648e214ebd9ee52925924b0524",
      "value": " 26%"
     }
    },
    "76bc31648e214ebd9ee52925924b0524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77cbb088fb97462c8c296cb15081308c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "995bc64903664453a7bd467773db0541": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_568a4ed4d64a491eb58a05ba3d791e0c",
       "IPY_MODEL_b988fb2118f2471db06c239711537eb4",
       "IPY_MODEL_181333071b524258879a7beb93a34553"
      ],
      "layout": "IPY_MODEL_e5fdde8faaa6426eb8f271f22289633e"
     }
    },
    "b988fb2118f2471db06c239711537eb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77cbb088fb97462c8c296cb15081308c",
      "max": 300,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0794419de3274e91b3c03cf443b2d4dd",
      "value": 78
     }
    },
    "d337fe9379d24d8ead564157b0830360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5fdde8faaa6426eb8f271f22289633e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
